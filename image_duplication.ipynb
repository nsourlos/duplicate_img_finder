{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26c3113",
   "metadata": {},
   "source": [
    "# Duplicate Image Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba083bb",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51505419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv venv image_duplicate_finder --python 3.11\n",
    "\n",
    "# #In Windows\n",
    "# .\\image_duplicate_finder\\Scripts\\activate\n",
    "\n",
    "# #In MacOS\n",
    "# source image_duplicate_finder/bin/activate\n",
    "\n",
    "# uv pip install ipykernel opencv-python pandas matplotlib tqdm\n",
    "\n",
    "# #In MacOS\n",
    "# image_duplicate_finder/bin/python -m ipykernel install --user --name=image_duplicate_finder --display-name \"image_duplicate_finder\"\n",
    "\n",
    "# #In Windows same as above but instead of 'bin' use 'Scripts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79266e",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Folder paths\n",
    "desktop_path = os.path.expanduser('~/Desktop')\n",
    "query_img_folder = os.path.join(desktop_path, \"image_folder\", 'queries') #Each image here appears as a duplicate in the source folder\n",
    "source_img_folder = os.path.join(desktop_path, \"image_folder\", 'sources') #Folder with original images\n",
    "test_query_img_folder = os.path.join(desktop_path, \"image_folder\", 'test_queries') #Test images in which groundtruth is provided\n",
    "\n",
    "#List of files in each folder\n",
    "query_img_files = os.listdir(query_img_folder)\n",
    "source_img_files = os.listdir(source_img_folder)\n",
    "test_query_img_files = os.listdir(test_query_img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c38595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c13c90525541b925ec5d390c9399faee.png</td>\n",
       "      <td>293.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5faef2c5413559dbed0c44220e259f16.png</td>\n",
       "      <td>427.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f18392f68dcb3bc6d827f9631601a0f.png</td>\n",
       "      <td>819.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35e53a47f4d581232df309469201797e.png</td>\n",
       "      <td>857.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d945350100c6442503a8e138692ba32d.png</td>\n",
       "      <td>235.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  query   source\n",
       "0  c13c90525541b925ec5d390c9399faee.png  293.png\n",
       "1  5faef2c5413559dbed0c44220e259f16.png  427.png\n",
       "2  0f18392f68dcb3bc6d827f9631601a0f.png  819.png\n",
       "3  35e53a47f4d581232df309469201797e.png  857.png\n",
       "4  d945350100c6442503a8e138692ba32d.png  235.png"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Groundtruth for test queries\n",
    "test_query_groundtruth_path = os.path.join(desktop_path, \"image_folder\", 'test_queries_groundtruth.csv')\n",
    "\n",
    "test_query_groundtruth = pd.read_csv(test_query_groundtruth_path)\n",
    "test_query_groundtruth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cacbe7d",
   "metadata": {},
   "source": [
    "### Create a figure to display each query image next to each source image to manually inspect cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58bed389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_query_source_pairs_figure(df=test_query_groundtruth, query_img_folder=test_query_img_folder, save_filename='test_query_source_pairs.png'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cv2\n",
    "\n",
    "    # Create a figure with subplots for each query-source pair\n",
    "    num_pairs = len(df)\n",
    "    fig, axes = plt.subplots(num_pairs, 2, figsize=(12, 6 * num_pairs))\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        query_filename = row['query']\n",
    "        source_filename = row['source']\n",
    "        \n",
    "        # Load query image\n",
    "        query_img_path = os.path.join(query_img_folder, query_filename)\n",
    "        query_img = cv2.imread(query_img_path)\n",
    "        query_img = cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for matplotlib\n",
    "        \n",
    "        # Load source image\n",
    "        source_img_path = os.path.join(source_img_folder, source_filename)\n",
    "        source_img = cv2.imread(source_img_path)\n",
    "        source_img = cv2.cvtColor(source_img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for matplotlib\n",
    "        \n",
    "        # Display query image\n",
    "        axes[idx, 0].imshow(query_img)\n",
    "        axes[idx, 0].set_title(f'Query: {query_filename}')\n",
    "        axes[idx, 0].axis('off')\n",
    "        \n",
    "        # Display source image\n",
    "        axes[idx, 1].imshow(source_img)\n",
    "        axes[idx, 1].set_title(f'Source: {source_filename}')\n",
    "        axes[idx, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.savefig(save_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78051c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_query_source_pairs_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42696835",
   "metadata": {},
   "source": [
    "### Feature Descriptors - Identify keypoints resistant to transformations (rotations, crops, partial deduplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f71d27",
   "metadata": {},
   "source": [
    "#### [SIFT](https://medium.com/@deepanshut041/introduction-to-sift-scale-invariant-feature-transform-65d7f3a72d40) (Scale-Invariant Feature Transform) internally does 2 things at the same time:\n",
    "\n",
    "1. **Keypoint detection**\n",
    "    - Image is repeatedly blurred with Gaussians at multiple σ scales. The 3D space of (x, y, scale) is separated into octaves (see image below) and the num of octaves depends on image size. Each octave size is half the previous one. Within each octave images are progressively blurred. \n",
    "\n",
    "    # <div style=\"text-align: center;\"><img src=\"imgs/scale.png\" height=\"600\"></div>\n",
    "\n",
    "    - For each scale, a difference-of-gaussians is computed as $$DoG(img, \\sigma_i) = G(\\sigma_{i+1}) - G(\\sigma_i)$$ This is done for different octaves of the images in the pyramid. \n",
    "    # <div style=\"text-align: center;\"><img src=\"imgs/DoG.png\" height=\"300\"></div>\n",
    "    - In the above 3D space, local maxima/minima are matching candidates. These maxima/minima are calculated by checking for each pixel in the DoG pyramid 26 neighbors:\n",
    "        - 8 in the same scale level (all pixels around it)\n",
    "        - 9 in scale below (same pixel in that scale and all around it)\n",
    "        - 9 in scale above\n",
    "\n",
    "        If the pixel values is bigger than all neighbors then it's a local maxima, and if it's smaller is a local minima. \n",
    "\n",
    "        # <div style=\"text-align: center;\"><img src=\"imgs/local_minima_maxima.png\" height=\"300\"></div>\n",
    "    - We filter out candidates with low contrast (intensity check) and those that lie along edges (similar to Harris Corner detector), since not useful as features.\n",
    "\n",
    "    The above procedure returns positions of keypoints, their scales and their local orientation. They are scale and rotation invariant.\n",
    "\n",
    "     <div style=\"text-align: center;\"><img src=\"imgs/keypoints.png\" height=\"600\"></div>\n",
    "    Stages of keypoint selection. (a) The 233x189 pixel original image. (b) The initial 832 keypoints locations at maxima and minima of the difference-of-Gaussian function. Keypoints are displayed as vectors indicating scale, orientation, and location. (c) After applying a threshold on minimum contrast, 729 keypoints remain. (d) The final 536 keypoints that remain following an additional threshold on ratio of principal curvatures.\n",
    "\n",
    "# \n",
    "\n",
    "2. **Descriptor Computation (for each keypoint)**\n",
    "    - Takes a region around the keypoint with radius proportional to its scale (higher scales result in larger regions - more pixels). For example, 3*σ.\n",
    "    - Rotate the chosen region (patch) to canonical dominant orientation. This is achieved by calculating gradients (dx, dy) around each keypoint and make a 36-bin orientation histogram with all gradients (depending on the gradient direction a certain keypoint is assigned to a bin - e.g. if is 19 degress it goes into the 10-19 degrees bin, and the amount added to that bin is proportional to the magnitude of gradient of that point). Then, SIFT rotates the patch so that the peak orientation in the histogram becomes angle=0 (also peaks above 80% considered to calculate orientation). That way we have rotation invariance since no matter how the image is rotated, the descriptor will always be computed in a normalized reference frame. \n",
    "    # <div style=\"text-align: center;\"><img src=\"imgs/histogram.png\" height=\"300\"></div>\n",
    "    - Divide the region into 4*4 subblocks\n",
    "    - For each sublock computes 8 orientation gradient histogram bins (0-360 degrees, every bin with 45 degrees). For each pixel we compute the gradient magnitude and direction. This contributes with a weighted magnitude to the nearest orientation bin(s) with Gaussian spatial weighting (center contributes more than edges). That gives the descriptor local smoothness and stability.\n",
    "    - Concatenate the results to get a vector of length 4 * 4 * 8=128 (descriptor). This uses gradient orientations and is not rotation invariant. Invariance is achieved by subtracting each orientation from each keypoint's rotation. Moreover, illumination independence is achived by thresholding. \n",
    "\n",
    "    # <div style=\"text-align: center;\"><img src=\"imgs/vector.png\" height=\"200\"></div>\n",
    "\n",
    "    Descriptor is stable across moderate lighting changes, noise, and small viewpoint changes. This is why this method is still the preferred one for many cases over DL solutions. \n",
    "\n",
    "\n",
    "Keypoints between two images are matched by identifying their nearest neighbor. In some cases, the 2nd closest match might be very near to the first due to noise. In that case, ratio of 1st/2nd distance is taken and if greater than a threshold (e.g. 0.8) the match is rejected. Based on the [paper](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf), this eliminates 90% of false matches, with the cost of discarding only 5% of correct ones. \n",
    "\n",
    " # <div style=\"text-align: center;\"><img src=\"imgs/example.png\" width=\"600\"></div>\n",
    "\n",
    "Overall, rotation invariance is achieved through canonical orientation normalization, and flipping and partial deduplication are achieved since SIFT is local and if part of images overlap, then keypoints will still match. For flipping, although SIFT is not inherently mirror-invariant, in practice it sometimes works since many gradients are symmetric and matcher does nearest neighbor search. That way, mirror invariance is 'incorporated' but not guaranteed. In our implementation, we test manually the flipped image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006e75a",
   "metadata": {},
   "source": [
    "#### Calculate source features and save them to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13117eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_source_features(feature_detector='SIFT', matcher='BF'): #takes ±10min for 2500 imgs for SIFT, 2min for ORB\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    import pickle\n",
    "\n",
    "    save_name = 'source_features_'+feature_detector+'_'+matcher+'.pkl'\n",
    "\n",
    "    # Initialize local feature detector\n",
    "    if feature_detector == 'SIFT':\n",
    "        sift = cv2.SIFT_create() # SIFT is great for robustness\n",
    "    elif feature_detector == 'ORB':\n",
    "        orb = cv2.ORB_create(nfeatures=2000) # ORB is faster - only approximate nearest neighbor\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid feature detector: {feature_detector}. Choose between 'SIFT' and 'ORB'.\")\n",
    "\n",
    "    # Initialize matcher - finds most similar descriptors between query-source imgs\n",
    "    if matcher == 'BF':\n",
    "        bf = cv2.BFMatcher() #For ORB and SIFT\n",
    "    elif matcher == 'FLANN':\n",
    "        flann = cv2.FlannBasedMatcher() #For SIFT\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid matcher: {matcher}. Choose between 'BF' and 'FLANN'.\")\n",
    "\n",
    "    # Pre-compute source features (to save time - optimization step)\n",
    "    source_features = {}\n",
    "    for source_path in tqdm(source_img_files, desc=\"Pre-computing source features\"):\n",
    "        # First convert to grayscale since feature descriptors work on such imgs and also help with contrast differences\n",
    "        img = cv2.imread(os.path.join(source_img_folder, source_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if feature_detector == 'SIFT':\n",
    "            kp, des = sift.detectAndCompute(img, None) \n",
    "        elif feature_detector == 'ORB':\n",
    "            kp, des = orb.detectAndCompute(img, None)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid feature detector: {feature_detector}. Choose between 'SIFT' and 'ORB'.\")\n",
    "        #The above returns the keypoints and descriptors. The second argument is region of img where detection is allowed (none to detect in entire img).\n",
    "        #We might not want to e.g. detect features in a region that corresponds to 'sky' and therefore we pass a binary mask to use the region of interest.\n",
    "\n",
    "        source_features[source_path] = (kp, des)\n",
    "        #kp is a list of cv2.KeyPoint objects. Each KeyPoint has necessarily the 'pt' attribute: Tuple of (x,y) coordinates of the keypoint ((num_keypoints, 2)).\n",
    "        #The other attributes (not used in our implementation) are:\n",
    "        #1) size: diameter of the neighborhood considered for this keypoint\n",
    "        #2) angle: orientation of the keypoint\n",
    "        #3) response: strength of the keypoint (measure of keypoint quality)\n",
    "        #4) octave: pyramid octave from which the keypoint was extracted\n",
    "        #These attributes would be needed if we wanted to e.g. visualize the keypoints, \n",
    "        # or use sift.compute() instead of sift.detectAndCompute() to get the keypoints and descriptors separately.\n",
    "        \n",
    "        #des is a numpy array of shape (n, 128) where n is the number of keypoints. Each row is the 128D descriptor for a keypoint.\n",
    "        \n",
    "    saved_source_features = {} #Convert to serializable format\n",
    "    # This assumes source_features is still populated in memory and contains (cv2.KeyPoint, descriptor array)\n",
    "    print(f\"Converting {len(source_features)} image feature sets...\")\n",
    "    for source_path, (kp, des) in source_features.items():\n",
    "        # Convert cv2.KeyPoint objects to a serializable NumPy array (only keeps the pt attribute, not the other ones)\n",
    "        kp_array = cv2.KeyPoint.convert(kp) \n",
    "        # Update the dictionary with the converted data\n",
    "        saved_source_features[source_path] = (kp_array, des)\n",
    "    print(\"Conversion complete.\")\n",
    "\n",
    "    try:\n",
    "        with open(save_name, 'wb') as f:\n",
    "            pickle.dump(saved_source_features, f)\n",
    "        print(f\"Saved source features to cache: {save_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving source features to cache: {e}\")\n",
    "\n",
    "    return source_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-computing source features: 100%|██████████| 2500/2500 [09:19<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 2500 image feature sets...\n",
      "Conversion complete.\n",
      "Saved source features to cache: source_features_SIFT_BF.pkl\n"
     ]
    }
   ],
   "source": [
    "# source_features_SIFT_BF = calculate_source_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159972bb",
   "metadata": {},
   "source": [
    "### Load pre-computed features and convert them back to opencv compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b543351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_source_features(feature_detector='SIFT', matcher='BF'):\n",
    "    import pickle\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    cache_file = 'source_features_'+feature_detector+'_'+matcher+'.pkl'\n",
    "\n",
    "    #Load from cache\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        loaded_source_features = pickle.load(f)\n",
    "    print(f\"Loaded source features from cache: {cache_file}\")\n",
    "\n",
    "    # Create a new dictionary to hold the OpenCV-compatible features\n",
    "    source_features = {}\n",
    "\n",
    "    for source_path, (kp_array, des) in loaded_source_features.items():\n",
    "        # Convert the NumPy array (kp_array) back to a list of cv2.KeyPoint objects\n",
    "        kp_objects = cv2.KeyPoint.convert(kp_array)\n",
    "        \n",
    "        # Store the results\n",
    "        source_features[source_path] = (kp_objects, des) #dict with key the source img paths and value the keypoints and descriptors\n",
    "        \n",
    "    print(\"Keypoints successfully converted back to cv2.KeyPoint objects.\")\n",
    "\n",
    "    return source_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15610421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded source features from cache: source_features_SIFT_BF.pkl\n",
      "Keypoints successfully converted back to cv2.KeyPoint objects.\n"
     ]
    }
   ],
   "source": [
    "source_features_SIFT_BF = load_source_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed30fb",
   "metadata": {},
   "source": [
    "### Find matches of queries to source imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5103c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(query_img_files=test_query_img_files, \n",
    "                 query_img_folder=test_query_img_folder, \n",
    "                 source_features=source_features_SIFT_BF,\n",
    "                 feature_detector='SIFT', \n",
    "                 matcher='BF', \n",
    "                 ransac_threshold=5, #Has to be at least 4 for findHomography to work\n",
    "                 acceptance_threshold=6,\n",
    "                 pixel_tolerance=5.0,\n",
    "                 lowe_ratio=0.75): #Takes 75mins for 50imgs\n",
    "                 \n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Initialize feature detector\n",
    "    if feature_detector == 'SIFT':\n",
    "        keypoint_detector = cv2.SIFT_create() # SIFT is great for robustness\n",
    "    elif feature_detector == 'ORB':\n",
    "        keypoint_detector = cv2.ORB_create(nfeatures=2000) # ORB is faster\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid feature detector: {feature_detector}. Choose between 'SIFT' and 'ORB'.\")\n",
    "\n",
    "    # Initialize matcher - finds most similar descriptors between query-source imgs\n",
    "    if matcher == 'BF' or feature_detector == 'ORB': #ORB cannot be directly used with FLANN\n",
    "        keypoint_matcher = cv2.BFMatcher() \n",
    "    elif matcher == 'FLANN' and feature_detector != 'ORB':\n",
    "        keypoint_matcher = cv2.FlannBasedMatcher()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid matcher: {matcher}. Choose between 'BF' and 'FLANN'.\")\n",
    "\n",
    "    # Don't run expensive RANSAC unless we have at least this many 'good' matches.\n",
    "    # If we lower this from e.g. 15 to 10 we can get a bit more permissive.\n",
    "    PRE_RANSAC_THRESHOLD = ransac_threshold\n",
    "\n",
    "    # If the *best* match we find still has fewer inliers than this, we consider it a \"no match\".\n",
    "    # Setting this to a low value (e.g., 8) will allow for weaker matches.\n",
    "    ACCEPTANCE_THRESHOLD = acceptance_threshold\n",
    "\n",
    "    # Main loop\n",
    "    final_matches = []\n",
    "    non_matched_queries = []\n",
    "    for query_path in tqdm(query_img_files, desc=\"Matching queries\"): #Loop over all query images to check for matches/duplicates\n",
    "        query_img = cv2.imread(os.path.join(query_img_folder, query_path), cv2.IMREAD_GRAYSCALE) #Read query image and convert to grayscale\n",
    "\n",
    "        best_match_for_this_query = (None, 0) # It will store (source_path, num_inliers_score) and keeps track of best match so far\n",
    "\n",
    "        # List of variants to check (original and flipped - SIFT is not invariant to flips)\n",
    "        variants_to_try = [\n",
    "            (\"original\", query_img),\n",
    "            (\"flipped horizontally\", cv2.flip(query_img, 1)) ,\n",
    "            # (\"flipped vertically\", cv2.flip(query_img, 0)),\n",
    "            # (\"flipped both\", cv2.flip(query_img, -1))\n",
    "            # 0 to flip vertically (mirror top↔bottom), +1 flip horizontally (mirror left↔right), and -1 to flip both axes (horizontal AND vertical) - similar to applying both flips\n",
    "            # vertical and both flips are not needed since SIFT is rotation invariant\n",
    "        ]\n",
    "\n",
    "        for variant_name, img_to_match in variants_to_try:\n",
    "            \n",
    "            # Get keypoints for the current variant (original or flipped)\n",
    "            q_kp, q_des = keypoint_detector.detectAndCompute(img_to_match, None)\n",
    "            if q_des is None:\n",
    "                continue  # This variant has no features/descriptors, try next one\n",
    "\n",
    "            # Loop over ALL sources\n",
    "            for source_path, (s_kp, s_des) in tqdm(source_features.items(), desc=f\"Matching {query_path} ({variant_name})\", leave=False): #leave=False to avoid extra prints\n",
    "                if s_des is None:\n",
    "                    continue\n",
    "                    \n",
    "                # Match descriptors - k=2 (two matches) for Lowe’s ratio test. The second nearest neighbor is needed in order to reject ambiguous matches (reduce FPs)\n",
    "                matches = keypoint_matcher.knnMatch(q_des, s_des, k=2) #cv2.DMatch objects - list of lists (with 2 elements - k=2 - , each having queryIdx, trainIdx, distance, etc.)\n",
    "                \n",
    "                # Filter matches (Lowe's Ratio Test)\n",
    "                good_matches = []\n",
    "                for m, n in matches: #best and second best match\n",
    "                    if m.distance < lowe_ratio * n.distance: #If the best match is much better than the second-best, then it is likely real. If both are similarly close → ambiguous → reject.\n",
    "                        good_matches.append(m)\n",
    "\n",
    "                # Geometric Verification (RANSAC) - Only run if we have enough good matches\n",
    "                if len(good_matches) >= PRE_RANSAC_THRESHOLD:\n",
    "                    \n",
    "                    #(x,y) keypoint matches and their locations in query/source images\n",
    "                    q_pts = np.float32([q_kp[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "                    s_pts = np.float32([s_kp[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "                    # RANSAC tries to find a geometric transform that maps query → source consistently. It tries random minimal subsets of correspondences.\n",
    "                    # For each subset, it solves for the homography that maps 2D points from query → source.\n",
    "                    # Then, it checks how many of the other correspondences agree with that homography (within an error threshold).\n",
    "                    # Matches that agree with that geometric model are inliers.\n",
    "                    # mask returned is a 0(outlier)/1(inlier) mask per match telling which are inliers. It has shape (n,1) where n is the number of matches.\n",
    "                    M, mask = cv2.findHomography(q_pts, s_pts, cv2.RANSAC, pixel_tolerance) #(srcPoints, dstPoints, method, ransacReprojThreshold)\n",
    "                    # Last parameter is pixel tolerance and it means that if a pixel is mapped using M lands more than 5 pixels away from expected, it is considered an outlier.\n",
    "                    # Decrease its value means fewer matches will be inliers => fewer false matches (do it if need very high certainty)\n",
    "                    # Increase its value means more matches will be inliers => more false matches (do it if missing real duplicate matches)\n",
    "                    \n",
    "                    if M is not None: #M should be a 3*3 homography matrix\n",
    "                        num_inliers = np.sum(mask) #Score - number of inliers\n",
    "                        inlier_ratio = num_inliers / len(good_matches)\n",
    "                        \n",
    "                        # Check if this match is better than the best one we've seen *so far* for this query (across original and flipped variants).\n",
    "                        if num_inliers > best_match_for_this_query[1] and inlier_ratio >= 0.40:\n",
    "                            best_match_for_this_query = (source_path, num_inliers) #Update the best match\n",
    "\n",
    "        # --- END OF INNER LOOPS (all sources and all variants checked) ---\n",
    "\n",
    "        # Now, we look at the single best match we found.\n",
    "        best_source_path, best_score = best_match_for_this_query\n",
    "\n",
    "        # Apply our final, low-threshold filter\n",
    "        if best_score >= ACCEPTANCE_THRESHOLD:\n",
    "            # The best match is good enough. Add it to the results.\n",
    "            final_matches.append((query_path, best_source_path))\n",
    "        else:\n",
    "            # The best match we found was still too weak (only few inliers).\n",
    "            non_matched_queries.append((query_path, best_source_path))\n",
    "\n",
    "    # Save results to pickle file\n",
    "    results_pickle_path = 'results_'+feature_detector+'_'+matcher+'_ransac'+str(ransac_threshold)+'_acc'+str(acceptance_threshold)+'.pkl'\n",
    "    with open(results_pickle_path, 'wb') as f:\n",
    "        pickle.dump(final_matches, f)\n",
    "    print(f\"Results saved to {results_pickle_path}\")\n",
    "\n",
    "    # Save non-matched queries to pickle file\n",
    "    non_matched_pickle_path = 'non_matched_queries_'+feature_detector+'_'+matcher+'_ransac'+str(ransac_threshold)+'_acc'+str(acceptance_threshold)+'.pkl'\n",
    "    with open(non_matched_pickle_path, 'wb') as f:\n",
    "        pickle.dump(non_matched_queries, f)\n",
    "    print(f\"Non-matched queries saved to {non_matched_pickle_path}\")\n",
    "\n",
    "    return final_matches, non_matched_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2abdd679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_validation(results):\n",
    "    correct_matches = []\n",
    "    for result in results:\n",
    "        row = test_query_groundtruth[test_query_groundtruth['query'] == result[0]]\n",
    "        print(row)\n",
    "        print(result)\n",
    "\n",
    "        if row['source'].values[0] == result[1]:\n",
    "            correct_matches.append(result)\n",
    "\n",
    "    print(\"Number of correct matches: \", len(correct_matches), \" out of \", len(results))\n",
    "\n",
    "    # Print query rows that are not in results_best\n",
    "    results_unique = set([result[0] for result in results])\n",
    "    missing_queries = test_query_groundtruth[~test_query_groundtruth['query'].isin(results_unique)]\n",
    "    print(\"Queries not in results_best:\")\n",
    "    print(missing_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212c3eb",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e758dd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|██████████| 50/50 [1:53:07<00:00, 135.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results_SIFT_BF_ransac5_acc6.pkl\n",
      "Non-matched queries saved to non_matched_queries_SIFT_BF_ransac5_acc6.pkl\n",
      "                                   query   source\n",
      "18  b023abd14c6600cfe66f684fdff137e6.png  240.png\n",
      "('b023abd14c6600cfe66f684fdff137e6.png', '240.png')\n",
      "                                   query   source\n",
      "28  e007132501dd130409defc6ba087c79e.png  779.png\n",
      "('e007132501dd130409defc6ba087c79e.png', '779.png')\n",
      "                                   query    source\n",
      "30  d2cb060508cd8364794f2cf0019d7b9c.png  1855.png\n",
      "('d2cb060508cd8364794f2cf0019d7b9c.png', '1855.png')\n",
      "                                   query    source\n",
      "41  7d9617443abc48dbc118d941fe8823e9.png  1935.png\n",
      "('7d9617443abc48dbc118d941fe8823e9.png', '1935.png')\n",
      "                                   query    source\n",
      "37  4ddd6b4e1310441d8dc0c396a223029c.png  1726.png\n",
      "('4ddd6b4e1310441d8dc0c396a223029c.png', '1726.png')\n",
      "                                   query   source\n",
      "29  771a413aa5dcc20125534b346e3c6fbb.png  266.png\n",
      "('771a413aa5dcc20125534b346e3c6fbb.png', '266.png')\n",
      "                                  query    source\n",
      "8  d9b22e09b75e3ed7b39a6761175c15b3.png  1355.png\n",
      "('d9b22e09b75e3ed7b39a6761175c15b3.png', '1355.png')\n",
      "                                   query   source\n",
      "19  8eae6fa4a8997f5cab103156a17204a9.png  910.png\n",
      "('8eae6fa4a8997f5cab103156a17204a9.png', '910.png')\n",
      "                                   query    source\n",
      "44  47b8b5b0a75c2e03a60d947b69031efc.png  2070.png\n",
      "('47b8b5b0a75c2e03a60d947b69031efc.png', '2070.png')\n",
      "                                  query   source\n",
      "0  c13c90525541b925ec5d390c9399faee.png  293.png\n",
      "('c13c90525541b925ec5d390c9399faee.png', '293.png')\n",
      "                                   query    source\n",
      "33  d78dc64962668bcca979a80181bec353.png  2216.png\n",
      "('d78dc64962668bcca979a80181bec353.png', '2216.png')\n",
      "                                   query    source\n",
      "45  8c58576c190caa52972d453d151c0be6.png  1470.png\n",
      "('8c58576c190caa52972d453d151c0be6.png', '1470.png')\n",
      "                                   query    source\n",
      "48  3a49c17288378f5629f625933ebaa8c9.png  1743.png\n",
      "('3a49c17288378f5629f625933ebaa8c9.png', '1743.png')\n",
      "                                  query   source\n",
      "3  35e53a47f4d581232df309469201797e.png  857.png\n",
      "('35e53a47f4d581232df309469201797e.png', '857.png')\n",
      "                                  query   source\n",
      "2  0f18392f68dcb3bc6d827f9631601a0f.png  819.png\n",
      "('0f18392f68dcb3bc6d827f9631601a0f.png', '819.png')\n",
      "                                   query    source\n",
      "46  604405c9ab232d22d9edbb216e3431e9.png  2065.png\n",
      "('604405c9ab232d22d9edbb216e3431e9.png', '2065.png')\n",
      "                                   query    source\n",
      "12  66b4b0facb9752f2d9c88902b09f5bcd.png  1004.png\n",
      "('66b4b0facb9752f2d9c88902b09f5bcd.png', '1004.png')\n",
      "                                   query    source\n",
      "31  1a73b708abcb3e0b181be0182d907946.png  1413.png\n",
      "('1a73b708abcb3e0b181be0182d907946.png', '1413.png')\n",
      "                                   query    source\n",
      "25  e9dbefa717eb6169ef63408945104acc.png  1858.png\n",
      "('e9dbefa717eb6169ef63408945104acc.png', '1858.png')\n",
      "                                  query    source\n",
      "9  d56a62300e82d604237eda3efa6b1c40.png  2359.png\n",
      "('d56a62300e82d604237eda3efa6b1c40.png', '2359.png')\n",
      "                                   query    source\n",
      "13  78ca43120ba12213e625e4f6361fe404.png  2361.png\n",
      "('78ca43120ba12213e625e4f6361fe404.png', '2361.png')\n",
      "                                   query    source\n",
      "32  5bf8b5599a8ad6050f2af44f1f4d2c8b.png  2137.png\n",
      "('5bf8b5599a8ad6050f2af44f1f4d2c8b.png', '2137.png')\n",
      "                                   query   source\n",
      "15  37f8b2634e2321272080619693e8cfe6.png  760.png\n",
      "('37f8b2634e2321272080619693e8cfe6.png', '760.png')\n",
      "                                   query  source\n",
      "38  880c477f5e31774a90fa1ead724e166c.png  35.png\n",
      "('880c477f5e31774a90fa1ead724e166c.png', '35.png')\n",
      "                                   query    source\n",
      "34  7bb781d45fca2bbb6b821134cb54209e.png  1443.png\n",
      "('7bb781d45fca2bbb6b821134cb54209e.png', '1443.png')\n",
      "                                   query    source\n",
      "39  9ec9cacbdda1738ddc0b5aca9c06916f.png  1447.png\n",
      "('9ec9cacbdda1738ddc0b5aca9c06916f.png', '1447.png')\n",
      "                                   query    source\n",
      "23  c52139a1094b95385c2a4cb2ad11036c.png  1681.png\n",
      "('c52139a1094b95385c2a4cb2ad11036c.png', '1681.png')\n",
      "                                   query    source\n",
      "35  c4f126ed69339c86728fda55ee96c90c.png  1941.png\n",
      "('c4f126ed69339c86728fda55ee96c90c.png', '1941.png')\n",
      "                                   query   source\n",
      "17  fb3bd204220847d77f040316f41b4f60.png  120.png\n",
      "('fb3bd204220847d77f040316f41b4f60.png', '120.png')\n",
      "                                   query    source\n",
      "36  5c2674fb4a2054609f14e06ee5916118.png  2347.png\n",
      "('5c2674fb4a2054609f14e06ee5916118.png', '2347.png')\n",
      "                                   query    source\n",
      "47  5533c2cb90fd57ac99d54fa03942ae38.png  2020.png\n",
      "('5533c2cb90fd57ac99d54fa03942ae38.png', '2020.png')\n",
      "                                   query   source\n",
      "20  9c26dbe06aed527673e9750a4f98741e.png  465.png\n",
      "('9c26dbe06aed527673e9750a4f98741e.png', '465.png')\n",
      "                                   query   source\n",
      "40  69f598c767207c3648057101414db61f.png  131.png\n",
      "('69f598c767207c3648057101414db61f.png', '131.png')\n",
      "                                   query    source\n",
      "24  7304c5d509f341cd9cfd63427403c1a0.png  2085.png\n",
      "('7304c5d509f341cd9cfd63427403c1a0.png', '2085.png')\n",
      "                                   query   source\n",
      "14  bdf71e68aa9c332c72def54f4e894552.png  964.png\n",
      "('bdf71e68aa9c332c72def54f4e894552.png', '964.png')\n",
      "                                   query    source\n",
      "27  198edaa7f3a082db47d4e2a6ca06249e.png  2407.png\n",
      "('198edaa7f3a082db47d4e2a6ca06249e.png', '2407.png')\n",
      "                                   query   source\n",
      "21  cec463f738e2da9331c2511a25dedd84.png  445.png\n",
      "('cec463f738e2da9331c2511a25dedd84.png', '445.png')\n",
      "                                  query    source\n",
      "6  98a5d34b9ec92938afe4a356d0cf89b0.png  1419.png\n",
      "('98a5d34b9ec92938afe4a356d0cf89b0.png', '1419.png')\n",
      "                                   query  source\n",
      "22  6c3cad5bbf2b23a15e5c599b6d7b0259.png  29.png\n",
      "('6c3cad5bbf2b23a15e5c599b6d7b0259.png', '29.png')\n",
      "                                   query    source\n",
      "16  2482948fbf725f3d08e725b669a503d4.png  1647.png\n",
      "('2482948fbf725f3d08e725b669a503d4.png', '1647.png')\n",
      "                                  query    source\n",
      "5  1c2bf79f1f914fc250046967c3111c33.png  1931.png\n",
      "('1c2bf79f1f914fc250046967c3111c33.png', '1931.png')\n",
      "                                   query   source\n",
      "42  b3730df6cd56aa8c8a2163bd436d048e.png  709.png\n",
      "('b3730df6cd56aa8c8a2163bd436d048e.png', '709.png')\n",
      "                                   query    source\n",
      "10  b29b2bb33e79a13b403cdbd13c29be8f.png  1267.png\n",
      "('b29b2bb33e79a13b403cdbd13c29be8f.png', '1267.png')\n",
      "                                   query    source\n",
      "11  0071b16da00dda50fad9e0fae67dfdac.png  1792.png\n",
      "('0071b16da00dda50fad9e0fae67dfdac.png', '1792.png')\n",
      "                                   query    source\n",
      "43  d8bdbc40962e647fe2985fde1567cbbf.png  1232.png\n",
      "('d8bdbc40962e647fe2985fde1567cbbf.png', '1232.png')\n",
      "                                   query   source\n",
      "49  41d0d4199737f4a2a149d217765301be.png  671.png\n",
      "('41d0d4199737f4a2a149d217765301be.png', '671.png')\n",
      "                                  query   source\n",
      "4  d945350100c6442503a8e138692ba32d.png  235.png\n",
      "('d945350100c6442503a8e138692ba32d.png', '235.png')\n",
      "                                  query   source\n",
      "1  5faef2c5413559dbed0c44220e259f16.png  427.png\n",
      "('5faef2c5413559dbed0c44220e259f16.png', '427.png')\n",
      "Number of correct matches:  48  out of  48\n",
      "Queries not in results_best:\n",
      "                                   query    source\n",
      "7   d3e3e6cb46086e23ed4e9140e99ad228.png  1458.png\n",
      "26  dc214392894266f271b4081bb9c7b67f.png  2210.png\n"
     ]
    }
   ],
   "source": [
    "results_SIFT_BF_ransac5_acc6, non_matched_queries_SIFT_BF_ransac5_acc6 = find_matches(source_features=source_features_SIFT_BF, \n",
    "                                                                                      feature_detector='SIFT', \n",
    "                                                                                      matcher='BF', \n",
    "                                                                                      ransac_threshold=5, \n",
    "                                                                                      acceptance_threshold=6)\n",
    "\n",
    "print_results_validation(results_SIFT_BF_ransac5_acc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e0d089d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d3e3e6cb46086e23ed4e9140e99ad228.png', None),\n",
       " ('dc214392894266f271b4081bb9c7b67f.png', None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_matched_queries_SIFT_BF_ransac5_acc6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9cb9e8",
   "metadata": {},
   "source": [
    "### Run Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24de8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching queries: 100%|██████████| 68/68 [26:44<00:00, 23.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results_SIFT_BF_ransac5_acc6.pkl\n",
      "Non-matched queries saved to non_matched_queries_SIFT_BF_ransac5_acc6.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_SIFT_BF_ransac5_acc6, non_matched_queries_predictions_SIFT_BF_ransac5_acc6 = find_matches(query_img_files=query_img_files, \n",
    "                                                                                                        query_img_folder=query_img_folder, \n",
    "                                                                                                        source_features=source_features_SIFT_BF, \n",
    "                                                                                                        feature_detector='SIFT',\n",
    "                                                                                                        matcher='BF', \n",
    "                                                                                                        ransac_threshold=5, \n",
    "                                                                                                        acceptance_threshold=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e61c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('61930a6c0bda1d7e39c250d980d0049a.png', '418.png'), ('09cdbb77dbfe80e1d3d5f516be8ef384.png', '954.png'), ('adfb62461764b18d39e679c576dab3d5.png', '1763.png'), ('deca309278f75dbd0e59908e8d4d0890.png', '807.png'), ('ac2293afb6315d376fe6c36936a03ace.png', '412.png'), ('72fa6a858675fe14d52da5953eeb4c4a.png', '1889.png'), ('7e8480b7431196acc9736ddd906fd867.png', '2143.png'), ('3946aa20504869d539cd0aaa73b85bcf.png', '561.png'), ('796ebc5612b058916c9563e69b8063dd.png', '1977.png'), ('b70eb89d5a3da01223a2f27f1c59ca2e.png', '1850.png'), ('fa17f8c1ab5566ded40692b0c7c62905.png', '1092.png'), ('62842a52e63081d4586aae865d537c09.png', '1972.png'), ('ac8b81c6cbea588c86b4d9c8604abace.png', '646.png'), ('ccb02781b6994092ef369b522f0f868c.png', '1812.png'), ('c533a992cf1bc1c00c51fe6004f7b1f5.png', '1506.png'), ('9763a4d7e70ca501140f35b342f1db30.png', '2194.png'), ('667cda67202350486984fc87645b0f31.png', '2232.png'), ('b50adb1f5f0e8aa5302b70ec2567fc47.png', '329.png'), ('8a1b27877911694b44cf514b598ce59d.png', '1687.png'), ('621501fa6eb5d90446fef08a0901f56c.png', '175.png'), ('83893ae1ab547393064cf93721ba905c.png', '1249.png'), ('643dc00bfb43e1b3a2d16361e0dc2e99.png', '1117.png'), ('1e833c0f3f917d3f501e4a97271742b0.png', '401.png'), ('71f0f1b36b3b3b08a044e6c6111702ac.png', '330.png'), ('eceb76fef56dfa2ff7fc4b5bbe58614d.png', '1957.png'), ('f6e238fb600b03c468475cdb7bd536fb.png', '2064.png'), ('63971c3a61f6b80b56af33e20cb9b363.png', '1903.png'), ('d576ca1123028a0da1ae1d99de613bcd.png', '1705.png'), ('e18818f01e49bbfcf8d942b1d8230ff3.png', '1462.png'), ('693c450735d059137d6b31b6eda3e1c4.png', '747.png'), ('4da936470b4cf363f986fff4a6849dbc.png', '966.png'), ('763178956166e829fa0e20148d6490fa.png', '2245.png'), ('a2bc0ca5edcfea1beee7912814cce8da.png', '774.png'), ('6156420ffbcae20623a2a5cae206435f.png', '946.png'), ('597a5eac3fcc395beb3a8ab9678c585b.png', '2057.png'), ('98e1e66c512bd3e046e19a3af374663d.png', '0.png'), ('c713a75aac17717a12c298d563f341ae.png', '526.png'), ('14a39c75e27d7dd5e7e3ff37097ef900.png', '735.png'), ('b0630629d115ab99d55caec201a83c4d.png', '527.png'), ('c31d7034a11df17b9b1ba330f362b169.png', '677.png'), ('9ce04a1a5bd9db2204a7263e8d0eacf6.png', '1746.png'), ('1c8995074285fdb6aeec1e504c67fc71.png', '2234.png'), ('389a08ae9834dc16b3188ec51434382c.png', '6.png'), ('09785fe16e1509755e2c7f9708efe88a.png', '1208.png'), ('5b7eb4a4a76bb9c73fded7bc6d63be3a.png', '2307.png'), ('1148a58f11869283c5f87d5b5848f1fa.png', '843.png'), ('a7de023de1c681dfb77250a4b4bd94d9.png', '1890.png')]\n",
      "[('7eea5aaadb3efc5376dfe4649c985373.png', None), ('0c1bb25fd6a1443eddfa0ce7097940a9.png', None), ('83a4b2b4a22fde843f7b3355cf6e9217.png', None), ('e408ecfd5c5f5b81278073b3d85a9442.png', None), ('612be6c5920d7255c75bc120814a05c8.png', None), ('ebc698333225aee6751fe1e8509031ed.png', None), ('87964aa6edd905673282f501f4b299c5.png', None), ('6665742bb5a3af3a04095dc632baa850.png', None), ('81c800b2c7d54eb29930159b7d4e679a.png', None), ('d4b33348126283138a5044d71f3ecf14.png', None), ('faf999923d59c32e16ca34951f72c608.png', '2058.png'), ('39259d211595117ecffc411a35569c6e.png', '1932.png'), ('b434f7cc691f8bbe75b2a8e80cbcd95a.png', None), ('ff4bcd56fde143bda360fe2264db9e11.png', None), ('0184a661f4080f2e42ed8f8c2c6d751e.png', None), ('f1c8c67fae0f86bdd6c69d8c16c47430.png', None), ('698583e90dd2ba58bb4ba6e35ad77e3b.png', None), ('d6d27e25cea92e179fc4dbfe8dd8ce4a.png', None), ('54ccf4b1f9f84313fb603ecd3f4e2713.png', '1515.png'), ('86c66affba63274d4bdee609b8414c0a.png', None), ('737dab49708b6038ee3cf5910e956e55.png', None)]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_SIFT_BF_ransac5_acc6)\n",
    "print(non_matched_queries_predictions_SIFT_BF_ransac5_acc6)\n",
    "#Create predictions dataframe\n",
    "predictions_df = pd.DataFrame(predictions_SIFT_BF_ransac5_acc6, columns=['query', 'source'])\n",
    "predictions_df.to_csv('predictions_SIFT_BF_ransac5_acc6.csv', index=False)\n",
    "save_query_source_pairs_figure(df=predictions_df, query_img_folder=query_img_folder, save_filename='predictions_source_pairs_SIFT_BF_ransac5_acc6.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duplicate_finder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
